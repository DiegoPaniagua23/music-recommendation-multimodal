# ğŸµ Arquitectura HÃ­brida Two-Tower para RecomendaciÃ³n Musical Multimodal

![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red)
![DVC](https://img.shields.io/badge/DVC-Data%20Version%20Control-purple)
![Status](https://img.shields.io/badge/Status-Development-yellow)

---

## ğŸ“– DescripciÃ³n General

Este proyecto implementa un sistema de recomendaciÃ³n musical del Estado del Arte (SOTA) utilizando una arquitectura **Two-Tower** con fusiÃ³n **Cross-Modal**. El objetivo es resolver los problemas de escasez de datos (*sparsity*) y brecha semÃ¡ntica (*semantic gap*) en los sistemas tradicionales.

El modelo alinea dos espacios vectoriales:
1.  **User Tower:** Codifica la secuencia histÃ³rica de interacciones del usuario usando **SASRec** (Transformer secuencial).
2.  **Item Tower:** Codifica el contenido de la canciÃ³n mediante **AtenciÃ³n Cruzada (Cross-Attention)** entre Audio (Mel-Spectrograms), Texto (Lyrics) e Imagen (CarÃ¡tulas).

## ğŸ—ï¸ Arquitectura del Sistema


## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

Este proyecto utiliza **`uv`** para la gestiÃ³n de dependencias y **DVC** para el control de versiones de datos.

### Prerrequisitos

  * Python 3.9+
  * [uv](https://github.com/astral-sh/uv) instalado.
  * `ffmpeg` instalado en el sistema (para procesamiento de audio).

### 1\. Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd proyecto-mir
```

### 2\. Instalar dependencias

```bash
uv sync
# Esto crearÃ¡ el entorno virtual e instalarÃ¡ todo lo necesario
```

### 3\. Configurar Datos (DVC)



### 4\. Variables de Entorno



-----

## ğŸ“‚ Estructura del Proyecto



-----

## ğŸ¤ Flujo de Trabajo Colaborativo



-----

## ğŸ‘¥ Equipo y Roles



## ğŸ“œ Licencia


