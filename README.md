# ğŸµ Arquitectura HÃ­brida Two-Tower para RecomendaciÃ³n Musical Multimodal

![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red)
![DVC](https://img.shields.io/badge/DVC-Data%20Version%20Control-purple)
![Status](https://img.shields.io/badge/Status-Development-yellow)

---

## ğŸ“– DescripciÃ³n General

Este proyecto implementa un sistema de recomendaciÃ³n musical del Estado del Arte (SOTA) utilizando una arquitectura **Two-Tower** con fusiÃ³n **Cross-Modal**. El objetivo es resolver los problemas de escasez de datos (*sparsity*) y brecha semÃ¡ntica (*semantic gap*) en los sistemas tradicionales.

El modelo alinea dos espacios vectoriales:
1.  **User Tower:** Codifica la secuencia histÃ³rica de interacciones del usuario usando **SASRec** (Transformer secuencial).
2.  **Item Tower:** Codifica el contenido de la canciÃ³n mediante **AtenciÃ³n Cruzada (Cross-Attention)** entre Audio (Mel-Spectrograms), Texto (Lyrics) e Imagen (CarÃ¡tulas).

## ğŸ—ï¸ Arquitectura del Sistema

![Arquitectura Two-Tower](report/images/architecture.png)

El sistema se basa en una arquitectura **Two-Tower** que aprende representaciones vectoriales (embeddings) tanto para usuarios como para Ã­tems en un espacio mÃ©trico compartido.

### 1. User Tower (Secuencial)
- **Entrada:** Secuencia histÃ³rica de interacciones del usuario (IDs de canciones).
- **Modelo:** **SASRec** (Self-Attention Sequential Recommendation).
- **Funcionamiento:** Utiliza mecanismos de auto-atenciÃ³n para capturar dependencias a largo y corto plazo en las preferencias del usuario.

### 2. Item Tower (Multimodal)
- **Entrada:** Audio, Texto (Letras) e ImÃ¡genes (CarÃ¡tulas).
- **Codificadores:**
    - **Audio:** ResNet-18 procesando Mel-Spectrograms.
    - **Texto:** mDeBERTa (con adaptadores LoRA) para procesar letras multilingÃ¼es.
    - **Imagen:** ResNet-18 pre-entrenada en ImageNet.
- **FusiÃ³n:** Mecanismo de **Cross-Attention** que permite a las modalidades interactuar y ponderar su importancia dinÃ¡micamente.

### 3. Entrenamiento
- **FunciÃ³n de PÃ©rdida:** **InfoNCE** (Contrastive Loss).
- **Objetivo:** Maximizar la similitud entre el embedding del usuario y el embedding del siguiente Ã­tem positivo, mientras se minimiza la similitud con Ã­tems negativos (in-batch negatives).

Este proyecto utiliza **`uv`** para la gestiÃ³n de dependencias y **DVC** para el control de versiones de datos.

### Prerrequisitos

  * Python 3.9+
  * [uv](https://github.com/astral-sh/uv) instalado.
  * `ffmpeg` instalado en el sistema (para procesamiento de audio).

### 1\. Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd proyecto-mir
```

### 2\. Instalar dependencias

```bash
uv sync
# Esto crearÃ¡ el entorno virtual e instalarÃ¡ todo lo necesario
```

### 3. Configurar Datos (DVC)

âš ï¸ **Aviso de Privacidad y Copyright:**
El dataset completo **no es de acceso pÃºblico**. Los archivos `.dvc` en este repositorio son solo metadatos (punteros). El almacenamiento remoto estÃ¡ restringido.

Para reproducir los experimentos, es necesario solicitar acceso a los autores. Si eres un colaborador autorizado, configura tus credenciales de Google Drive:

```bash
# Configurar credenciales locales (no se suben al repo, se adjuntan en el trabajo)
dvc remote modify --local proyecto_multimodal gdrive_client_id "<gdrive_client_id>"
dvc remote modify --local proyecto_multimodal gdrive_client_secret "<gdrive_client_secret>"

# Descargar datos
uv run dvc pull
```

**Nota:** En el futuro, se publicarÃ¡ una versiÃ³n del dataset libre de derechos que incluirÃ¡ Ãºnicamente los embeddings pre-calculados y features extraÃ­dos.

### 4. Variables de Entorno

AsegÃºrate de tener configuradas las variables necesarias, especialmente si usas modelos de HuggingFace que requieran token (aunque mDeBERTa es pÃºblico).

```bash
export HF_HOME="./.cache/huggingface"
```

-----

## ğŸ‹ï¸â€â™‚ï¸ Entrenamiento

Para entrenar el modelo desde cero hemos utilizado el clÃºster del CIMAT (BajÃ­o), el cual cuenta con 2 GPUs en cada nodo. El funcionamiento puede variar dependiendo del hardware donde se quiera reproducir el entrenamiento (Los requerimientos de Hardware son altos, un tamaÃ±o de lote de 64 requiere mas de 24 GB VRAM).

En todo caso, utilizamos el script `src/train.py`. Este script se encarga de:
1. Cargar y preprocesar los datos.
2. Ajustar y guardar los encoders (necesarios para inferencia).
3. Entrenar el modelo Two-Tower.

```bash
# Agregar el directorio actual al PYTHONPATH para que Python encuentre el mÃ³dulo 'src'
export PYTHONPATH=$PWD

# Para el clÃºster CIMAT utilizamos el script train.sh
uv run python -m torch.distributed.run --nproc_per_node=1 src/train.py \
    --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
    --img_dir "data/spotify-kaggle/album_covers/" \
    --audio_dir "data/audio/mels/" \
    --lyrics_path "data/spotify-kaggle/interim/lyrics_dataset_10k_fixed.csv" \
    --epochs 10 \
    --batch_size 32

# En local (considerando una GPU).
uv run python -m torch.distributed.run --nproc_per_node=1 src/train.py \
    --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
    --img_dir "data/spotify-kaggle/album_covers/" \
    --audio_dir "data/audio/mels/" \
    --lyrics_path "data/spotify-kaggle/interim/lyrics_dataset_10k_fixed.csv" \
    --epochs 10 \
    --batch_size 32
```

**Nota:** Los checkpoints y encoders se guardarÃ¡n automÃ¡ticamente en la carpeta `checkpoints/`.

-----

## ğŸ“Š EvaluaciÃ³n

Para evaluar el rendimiento del modelo (Recall@K, NDCG@K) sobre el conjunto de validaciÃ³n/test:

```bash
uv run python src/evaluate_metrics.py \
  --model_path "checkpoints/complete/best_model_epoch1.pth" \
  --encoders_path "checkpoints/complete/encoders.pkl" \
  --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
  --embeddings_cache_path "checkpoints/complete/item_embeddings_cache_epoch1.pt" \
  --batch_size 32
```

-----

## ğŸ”® Inferencia y RecomendaciÃ³n

El sistema de inferencia tiene dos modos: **IndexaciÃ³n** y **RecomendaciÃ³n**.

### Paso 1: IndexaciÃ³n (`index`)
Pre-calcula los embeddings de todas las canciones del catÃ¡logo para una bÃºsqueda rÃ¡pida.

```bash
uv run python -m src.inference \
  --mode index \
  --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
  --mapper_path "data/spotify-kaggle/interim/item_id_mapper.json" \
  --model_path "checkpoints/complete/best_model_epoch1.pth" \
  --encoders_path "checkpoints/complete/encoders.pkl" \
  --index_path "checkpoints/complete/item_index_epoch1.pt"
```

### Paso 2: RecomendaciÃ³n (`recommend`)
Genera recomendaciones personalizadas para un usuario especÃ­fico basÃ¡ndose en su historial.

```bash
uv run python -m src.inference \
  --mode recommend \
  --user_id "user_000232" \
  --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
  --mapper_path "data/spotify-kaggle/interim/item_id_mapper.json" \
  --model_path "checkpoints/complete/best_model_epoch1.pth" \
  --encoders_path "checkpoints/complete/encoders.pkl" \
  --index_path "checkpoints/complete/item_index_epoch1.pt"
```

-----

## ğŸ“‚ Estructura del Proyecto

```
.
â”œâ”€â”€ checkpoints/        # Punteros DVC (.dvc) a modelos y encoders
â”œâ”€â”€ notebooks/          # Jupyter Notebooks para EDA y prototipado
â”œâ”€â”€ report/             # CÃ³digo fuente LaTeX del reporte tÃ©cnico
â”‚   â”œâ”€â”€ chapters/       # CapÃ­tulos del reporte
â”‚   â””â”€â”€ images/         # Figuras y diagramas
â”œâ”€â”€ src/                # CÃ³digo fuente del sistema
â”‚   â”œâ”€â”€ data/           # Scripts de descarga y procesamiento
â”‚   â”œâ”€â”€ models/         # Arquitecturas (TwoTower, Encoders)
â”‚   â”œâ”€â”€ scripts/        # Scripts de utilidad (check embeddings, download)
â”‚   â”œâ”€â”€ utils/          # Funciones auxiliares (monitor, stats)
â”‚   â”œâ”€â”€ dataset.py      # Clase MultimodalDataset
â”‚   â”œâ”€â”€ evaluate_metrics.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ data.dvc            # Puntero DVC al dataset (audio, imÃ¡genes, metadatos)
â”œâ”€â”€ logs.dvc            # Puntero DVC a los logs de ejecuciÃ³n
â”œâ”€â”€ papers.dvc          # Puntero DVC a referencias bibliogrÃ¡ficas
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml      # ConfiguraciÃ³n de dependencias (uv)
â””â”€â”€ uv.lock
```

-----

## ğŸ¤ Flujo de Trabajo Colaborativo

Este proyecto sigue las mejores prÃ¡cticas de MLOps para garantizar la reproducibilidad y la colaboraciÃ³n efectiva:

- **CÃ³digo:** Control de versiones con **Git** y **GitHub**.
- **Datos:** Versionado de grandes volÃºmenes de datos (audio, imÃ¡genes) con **DVC** (Data Version Control) y almacenamiento remoto en Google Drive.
- **Dependencias:** GestiÃ³n determinista de paquetes con **`uv`**.
- **ExperimentaciÃ³n:** Registro de mÃ©tricas y modelos (checkpoints).

-----

## ğŸ‘¥ Equipo y Roles

Este proyecto fue desarrollado como parte del programa de MaestrÃ­a en CÃ³mputo EstadÃ­stico en el **Centro de InvestigaciÃ³n en MatemÃ¡ticas (CIMAT)**, Unidad Monterrey.

*   **CÃ©sar Aguirre-Calzadilla** - [cesar.aguirre@cimat.mx](mailto:cesar.aguirre@cimat.mx)
*   **Gustavo HernÃ¡ndez-Angeles** - [gustavo.hernandez@cimat.mx](mailto:gustavo.hernandez@cimat.mx)
*   **Diego Paniagua-Molina** - [diego.paniagua@cimat.mx](mailto:diego.paniagua@cimat.mx)

## ğŸ“œ Licencia

Este proyecto estÃ¡ bajo la Licencia **MIT**. Consulta el archivo `LICENSE` para mÃ¡s detalles.

El reporte y contenido acadÃ©mico se distribuye bajo la licencia **Creative Commons Attribution 4.0 International (CC BY 4.0)**.


