# üéµ Arquitectura H√≠brida Two-Tower para Recomendaci√≥n Musical Multimodal

![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red)
![DVC](https://img.shields.io/badge/DVC-Data%20Version%20Control-purple)
![Status](https://img.shields.io/badge/Status-Development-yellow)

---

## üìñ Descripci√≥n General

Este proyecto implementa un sistema de recomendaci√≥n musical del Estado del Arte (SOTA) utilizando una arquitectura **Two-Tower** con fusi√≥n **Cross-Modal**. El objetivo es resolver los problemas de escasez de datos (*sparsity*) y brecha sem√°ntica (*semantic gap*) en los sistemas tradicionales.

El modelo alinea dos espacios vectoriales:
1.  **User Tower:** Codifica la secuencia hist√≥rica de interacciones del usuario usando **SASRec** (Transformer secuencial).
2.  **Item Tower:** Codifica el contenido de la canci√≥n mediante **Atenci√≥n Cruzada (Cross-Attention)** entre Audio (Mel-Spectrograms), Texto (Lyrics) e Imagen (Car√°tulas).

## üèóÔ∏è Arquitectura del Sistema

El sistema se basa en una arquitectura **Two-Tower** que aprende representaciones vectoriales (embeddings) tanto para usuarios como para √≠tems en un espacio m√©trico compartido.

### 1. User Tower (Secuencial)
- **Entrada:** Secuencia hist√≥rica de interacciones del usuario (IDs de canciones).
- **Modelo:** **SASRec** (Self-Attention Sequential Recommendation).
- **Funcionamiento:** Utiliza mecanismos de auto-atenci√≥n para capturar dependencias a largo y corto plazo en las preferencias del usuario.

### 2. Item Tower (Multimodal)
- **Entrada:** Audio, Texto (Letras) e Im√°genes (Car√°tulas).
- **Codificadores:**
    - **Audio:** ResNet-18 procesando Mel-Spectrograms.
    - **Texto:** mDeBERTa (con adaptadores LoRA) para procesar letras multiling√ºes.
    - **Imagen:** ResNet-18 pre-entrenada en ImageNet.
- **Fusi√≥n:** Mecanismo de **Cross-Attention** que permite a las modalidades interactuar y ponderar su importancia din√°micamente.

### 3. Entrenamiento
- **Funci√≥n de P√©rdida:** **InfoNCE** (Contrastive Loss).
- **Objetivo:** Maximizar la similitud entre el embedding del usuario y el embedding del siguiente √≠tem positivo, mientras se minimiza la similitud con √≠tems negativos (in-batch negatives).

Este proyecto utiliza **`uv`** para la gesti√≥n de dependencias y **DVC** para el control de versiones de datos.

### Prerrequisitos

  * Python 3.9+
  * [uv](https://github.com/astral-sh/uv) instalado.
  * `ffmpeg` instalado en el sistema (para procesamiento de audio).

### 1\. Clonar el repositorio

```bash
git clone <url-del-repositorio>
cd proyecto-mir
```

### 2\. Instalar dependencias

```bash
uv sync
# Esto crear√° el entorno virtual e instalar√° todo lo necesario
```

### 3. Configurar Datos (DVC)

Para descargar los datos, necesitas configurar las credenciales de Google Drive. Ejecuta los siguientes comandos:

```bash
# Configurar credenciales locales (no se suben al repo, se adjuntan en el trabajo)
dvc remote modify --local proyecto_multimodal gdrive_client_id "<gdrive_client_id>"
dvc remote modify --local proyecto_multimodal gdrive_client_secret "<gdrive_client_secret>"

# Descargar datos
uv run dvc pull
```

### 4. Variables de Entorno

Aseg√∫rate de tener configuradas las variables necesarias, especialmente si usas modelos de HuggingFace que requieran token (aunque mDeBERTa es p√∫blico).

```bash
export HF_HOME="./.cache/huggingface"
```

-----

## üèãÔ∏è‚Äç‚ôÇÔ∏è Entrenamiento

Para entrenar el modelo desde cero hemos utilizado el cl√∫ster del CIMAT (Baj√≠o), el cual cuenta con 2 GPUs en cada nodo. El funcionamiento puede variar dependiendo del hardware donde se quiera reproducir el entrenamiento (Los requerimientos de Hardware son altos, un tama√±o de lote de 64 requiere mas de 24 GB VRAM).

En todo caso, utilizamos el script `src/train.py`. Este script se encarga de:
1. Cargar y preprocesar los datos.
2. Ajustar y guardar los encoders (necesarios para inferencia).
3. Entrenar el modelo Two-Tower.

```bash
# Agregar el directorio actual al PYTHONPATH para que Python encuentre el m√≥dulo 'src'
export PYTHONPATH=$PWD

# Para el cl√∫ster CIMAT utilizamos el script train.sh
uv run python -m torch.distributed.run --nproc_per_node=1 src/train.py \
    --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
    --img_dir "data/spotify-kaggle/album_covers/" \
    --audio_dir "data/audio/mels/" \
    --lyrics_path "data/spotify-kaggle/interim/lyrics_dataset_10k_fixed.csv" \
    --epochs 10 \
    --batch_size 32

# En local (considerando una GPU).
uv run python -m torch.distributed.run --nproc_per_node=1 src/train.py \
    --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
    --img_dir "data/spotify-kaggle/album_covers/" \
    --audio_dir "data/audio/mels/" \
    --lyrics_path "data/spotify-kaggle/interim/lyrics_dataset_10k_fixed.csv" \
    --epochs 10 \
    --batch_size 32
```

**Nota:** Los checkpoints y encoders se guardar√°n autom√°ticamente en la carpeta `checkpoints/`.

-----

## üìä Evaluaci√≥n

Para evaluar el rendimiento del modelo (Recall@K, NDCG@K) sobre el conjunto de validaci√≥n/test:

```bash
uv run python src/evaluate_metrics.py \
  --model_path "checkpoints/complete/best_model_epoch1.pth" \
  --encoders_path "checkpoints/complete/encoders.pkl" \
  --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
  --embeddings_cache_path "checkpoints/complete/item_embeddings_cache_epoch1.pt" \
  --batch_size 32
```

-----

## üîÆ Inferencia y Recomendaci√≥n

El sistema de inferencia tiene dos modos: **Indexaci√≥n** y **Recomendaci√≥n**.

### Paso 1: Indexaci√≥n (`index`)
Pre-calcula los embeddings de todas las canciones del cat√°logo para una b√∫squeda r√°pida.

```bash
uv run python -m src.inference \
  --mode index \
  --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
  --mapper_path "data/spotify-kaggle/interim/item_id_mapper.json" \
  --model_path "checkpoints/complete/best_model_epoch1.pth" \
  --encoders_path "checkpoints/complete/encoders.pkl" \
  --index_path "checkpoints/complete/item_index_epoch1.pt"
```

### Paso 2: Recomendaci√≥n (`recommend`)
Genera recomendaciones personalizadas para un usuario espec√≠fico bas√°ndose en su historial.

```bash
uv run python -m src.inference \
  --mode recommend \
  --user_id "user_000232" \
  --data_path "data/spotify-kaggle/interim/lastfm_spotify_merged.csv" \
  --mapper_path "data/spotify-kaggle/interim/item_id_mapper.json" \
  --model_path "checkpoints/complete/best_model_epoch1.pth" \
  --encoders_path "checkpoints/complete/encoders.pkl" \
  --index_path "checkpoints/complete/item_index_epoch1.pt"
```

-----

## üìÇ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ data/               # Datos crudos y procesados (gestionado por DVC)
‚îú‚îÄ‚îÄ notebooks/          # Jupyter Notebooks para EDA y prototipado
‚îú‚îÄ‚îÄ src/                # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py      # Clase MultimodalDataset y l√≥gica de carga
‚îÇ   ‚îú‚îÄ‚îÄ models/         # Definici√≥n de arquitecturas (TwoTower, Encoders)
‚îÇ   ‚îú‚îÄ‚îÄ train.py        # Script de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ inference.py    # Script de inferencia y recomendaci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ evaluate_metrics.py # Script de evaluaci√≥n
‚îú‚îÄ‚îÄ checkpoints/        # Modelos entrenados y encoders guardados
‚îú‚îÄ‚îÄ pyproject.toml      # Dependencias y configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ uv.lock             # Lockfile de dependencias
```

-----

## ü§ù Flujo de Trabajo Colaborativo

Este proyecto sigue las mejores pr√°cticas de MLOps para garantizar la reproducibilidad y la colaboraci√≥n efectiva:

- **C√≥digo:** Control de versiones con **Git** y **GitHub**.
- **Datos:** Versionado de grandes vol√∫menes de datos (audio, im√°genes) con **DVC** (Data Version Control) y almacenamiento remoto en Google Drive.
- **Dependencias:** Gesti√≥n determinista de paquetes con **`uv`**.
- **Experimentaci√≥n:** Registro de m√©tricas y modelos (checkpoints).

-----

## üë• Equipo y Roles

Este proyecto fue desarrollado como parte del programa de Maestr√≠a en C√≥mputo Estad√≠stico en el **Centro de Investigaci√≥n en Matem√°ticas (CIMAT)**, Unidad Monterrey.

*   **C√©sar Aguirre-Calzadilla** - [cesar.aguirre@cimat.mx](mailto:cesar.aguirre@cimat.mx)
*   **Gustavo Hern√°ndez-Angeles** - [gustavo.hernandez@cimat.mx](mailto:gustavo.hernandez@cimat.mx)
*   **Diego Paniagua-Molina** - [diego.paniagua@cimat.mx](mailto:diego.paniagua@cimat.mx)

## üìú Licencia

Este proyecto est√° bajo la Licencia **MIT**. Consulta el archivo `LICENSE` para m√°s detalles.

El reporte y contenido acad√©mico se distribuye bajo la licencia **Creative Commons Attribution 4.0 International (CC BY 4.0)**.


